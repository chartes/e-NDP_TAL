{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42ESci7mCMI1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from xml.etree import ElementTree as ET\n",
        "import re\n",
        "import xml.dom.minidom\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "import cv2\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT7Nu5DKWyOx"
      },
      "outputs": [],
      "source": [
        "!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JShJdvODbVTw"
      },
      "outputs": [],
      "source": [
        "!wget https://gitlab.com/magistermilitum/ner_medieval_multilingual/-/raw/main/datasets/all_latin_texts_diplo.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZexBuLmCRiF"
      },
      "outputs": [],
      "source": [
        "#orden para instalar las paginas xml en una carpeta pages\n",
        "!wget https://gitlab.com/magistermilitum/e-ndp_htr/-/raw/main/page/all_manual_transcriptions.zip\n",
        "zip_url=\"/content/all_manual_transcriptions.zip\"\n",
        "\n",
        "!mkdir all_manual_transcriptions\n",
        "!unzip $zip_url -d /content/all_manual_transcriptions/\n",
        "\n",
        "!wget https://gitlab.com/magistermilitum/e-ndp_htr/-/raw/main/page/endp_pages_all_V7.zip #14k PAGE-XML files\n",
        "!mkdir all_transcriptions\n",
        "zip_url=\"/content/endp_pages_all_V7.zip\"\n",
        "!unzip $zip_url -d /content/all_transcriptions/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goXyMjGUVoIu"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import unicodedata\n",
        "import editdistance\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):\n",
        "    \"\"\"Calculate Character Error Rate (CER), Word Error Rate (WER) and Sequence Error Rate (SER)\"\"\"\n",
        "\n",
        "    if len(predicts) == 0 or len(ground_truth) == 0:\n",
        "        return (1, 1, 1)\n",
        "\n",
        "    cer, wer, ser = [], [], []\n",
        "\n",
        "    for (pd, gt) in zip(predicts, ground_truth):\n",
        "        pd, gt = pd.lower(), gt.lower()\n",
        "\n",
        "        if norm_accentuation:\n",
        "            pd = unicodedata.normalize(\"NFKD\", pd).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
        "            gt = unicodedata.normalize(\"NFKD\", gt).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
        "\n",
        "        if norm_punctuation:\n",
        "            pd = pd.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "            gt = gt.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "        pd_cer, gt_cer = list(pd), list(gt)\n",
        "        #print(pd_cer)\n",
        "        dist = editdistance.eval(pd_cer, gt_cer)\n",
        "        cer.append(dist / (max(len(pd_cer), len(gt_cer))))\n",
        "\n",
        "        pd_wer, gt_wer = pd.split(), gt.split()\n",
        "        dist = editdistance.eval(pd_wer, gt_wer)\n",
        "        wer.append(dist / (max(len(pd_wer), len(gt_wer))))\n",
        "\n",
        "        pd_ser, gt_ser = [pd], [gt]\n",
        "        dist = editdistance.eval(pd_ser, gt_ser)\n",
        "        ser.append(dist / (max(len(pd_ser), len(gt_ser))))\n",
        "\n",
        "    metrics = [cer, wer, ser]\n",
        "    metrics = np.mean(metrics, axis=1)\n",
        "\n",
        "    return round(list(metrics)[0], 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lvnKPlIdCRkf"
      },
      "outputs": [],
      "source": [
        "#nube de puntos a coordenadas rectangulares\n",
        "def square(h):\n",
        "  h=list(map(int, h.replace(\",\", \" \").split(\" \")))\n",
        "  h=list(zip(*[iter(h)]*2))\n",
        "  h=np.array([h])\n",
        "  h=cv2.boundingRect(h) #X coordinate, Y coordinate, width, height\n",
        "  return h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMlbppo6CRoW"
      },
      "outputs": [],
      "source": [
        "#guarda diccionario con id, puntos y contenido\n",
        "\n",
        "nnn=[]\n",
        "dict_unicodes={} #solo los textos\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "dict_registres={} #id, puntos y texto de cada linea\n",
        "dict_regions={} #id, puntos y tipo de cada region\n",
        "\n",
        "#for filename in glob.glob(\"/content/108_a/*.xml\"):\n",
        "#for filename in glob.glob(\"/content/all_transcriptions/raw_plus_manual/*.xml\"):\n",
        "for filename in glob.glob(\"/content/all_manual_transcriptions/*.xml\"):\n",
        "  nombre=filename.split(\"/\")[-1].split(\".\")[0]\n",
        "  tree=ET.parse(filename)\n",
        "  root=tree.getroot()\n",
        "\n",
        "  ids=[elem.attrib[\"id\"] for elem in tree.findall('.//{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}TextLine')]\n",
        "  points=[point.attrib[\"points\"] for point in tree.findall('.//{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}Baseline')]\n",
        "  nnn.append(points)\n",
        "\n",
        "  unicodes=[uni.text for uni in tree.findall('.//{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}Unicode')]\n",
        "\n",
        "  ids_regions=[elem.attrib[\"id\"] for elem in tree.findall('.//{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}TextRegion')]\n",
        "  #points_regions=[point for point in tree.findall(\".//{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}TextRegion\")]\n",
        "\n",
        "  points_regions=[]#coordenadas de cada region\n",
        "  for point in tree.findall(\".//{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}TextRegion\"):\n",
        "    for y in point:\n",
        "      try:\n",
        "        points_regions.append(y.attrib[\"points\"])\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "\n",
        "  #for address_ip in tree.find('sub2').find('source-IP').findall('address-IP'):\n",
        "\n",
        "\n",
        "  type_regions=[]\n",
        "  for elem  in tree.findall('.//{http://schema.primaresearch.org/PAGE/gts/pagecontent/2019-07-15}TextRegion'):\n",
        "    try:\n",
        "      type_regions.append(elem.attrib[\"custom\"])\n",
        "    except:\n",
        "      type_regions.append(\"-\")\n",
        "\n",
        "  #dict_unicodes[nombre]=[x for x in unicodes if x!=None]\n",
        "  xml_content=list(zip(ids, points, unicodes))#tupla: ids, baseline, text\n",
        "  xml_regions=list(zip(ids_regions, points_regions, type_regions))\n",
        "\n",
        "  if \"5705\" in filename:\n",
        "    print(xml_content)\n",
        "\n",
        "  dict_registres[nombre]=[[x[0], square(x[1]), x[2]] for x in xml_content if type(x[2])==str] #contiene: nombre de la linea, coordenadas square y texto de linea\n",
        "  dict_regions[nombre]=[[x[0], square(x[1]), x[2]] for x in xml_regions if x[2]!=\"-\" ]#contiene: nombre de la region, coordenadas square y typo de region\n",
        "\n",
        "ordenada=sorted(dict_registres.keys())[35:]+sorted(dict_registres.keys())[:35]\n",
        "\n",
        "dict_registres={k:dict_registres[k] for k in ordenada}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQvLxHkTXlMI"
      },
      "outputs": [],
      "source": [
        "# Function to transform XML extracted content to a single python dictionary\n",
        "def transformation (filename):\n",
        "  #print(filename)\n",
        "  regions_list=[\"block\", \"Date\", \"liste\", \"entrée\", \"numérotation\"] # the five trained regions\n",
        "\n",
        "  dict_html={k:[] for k in regions_list} #dictionnary with regions as keys and list of text lines as values\n",
        "  lista_html=[]\n",
        "\n",
        "  for i, elem in enumerate(dict_registres[filename]): \n",
        "    # FOR THE LINES\n",
        "    longueur=elem[1][0]+elem[1][2] #width starting point + size\n",
        "    hauteur=elem[1][1]+elem[1][3] #height starting point + size\n",
        "    central_point=(elem[1][0]+(elem[1][2]/2), elem[1][1]+round((elem[1][3]/3)), elem[1][1]+round((elem[1][3]/3))+round((elem[1][3]/3))) # squared line central point\n",
        "   \n",
        "    # FOR THE REGIONS\n",
        "    region_sorted=sorted(dict_regions[filename], key=lambda l: l[1][2]-l[1][0])#some blocks are inside others\n",
        "    for x in region_sorted:\n",
        "      longueur_reg=x[1][0]+x[1][2] #width starting point + size\n",
        "      hauteur_reg=x[1][1]+x[1][3] #height starting point + size\n",
        "      central_point_reg=(x[1][0]+(x[1][2]/2), x[1][1]+(x[1][3]/2)) # squared region central point\n",
        "\n",
        "     \n",
        "      # Condition: We must find in wich region the central point of each line is localised\n",
        "      if central_point[0]>x[1][0] and central_point[0]<x[1][0]+x[1][2] and central_point[1]>x[1][1] and central_point[1]<x[1][3]+x[1][1] and central_point[2]>x[1][1]:\n",
        "\n",
        "      #if longueur<=longueur_reg and hauteur<=hauteur_reg:\n",
        "        #print(x[2], \"\\n\", elem, \"\\n\\n\" )\n",
        "        dict_html[x[2].split(\"type:\")[1].split(\";\")[0]].append(elem[2])\n",
        "        lista_html.append([i, x[2].split(\"type:\")[1].split(\";\")[0], elem[2]])\n",
        "        break\n",
        "\n",
        "  return lista_html, dict_html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp8CxAkkXq71"
      },
      "outputs": [],
      "source": [
        "#Testing\n",
        "#Warning: its mandatory use a deepcopy element in order to avoid loop modifications over iterations\n",
        "import copy\n",
        "a,b=transformation(list(dict_registres.keys())[10000])\n",
        "c=copy.deepcopy(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bl55VjCYuAL"
      },
      "outputs": [],
      "source": [
        "dict_registres[\"FRAN_0393_01725_L\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOGQfi7MX9M-"
      },
      "outputs": [],
      "source": [
        "a,b=transformation(list(dict_registres.keys())[1])\n",
        "d=copy.deepcopy(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn7xnbKKXtMW"
      },
      "outputs": [],
      "source": [
        "\" \".join(c[\"block\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY5__3ExYKH2"
      },
      "outputs": [],
      "source": [
        "d[\"block\"][3:12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GyiRPmiXTy_",
        "outputId": "b304d25d-18f0-4e9d-d7bb-15995ec58958"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9829"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "1-ocr_metrics(c[\"block\"][3:12], d[\"block\"][3:12])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87N74mVNXamN"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "test=[]\n",
        "diccionario=[]\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "#with open(\"pretrain_src1_all_small.txt\") as f:\n",
        "with open(\"/content/all_latin_texts_diplo.txt\") as f:\n",
        "  f=f.readlines()\n",
        "  f=[word_tokenize(x[:-1]) for x in f ]\n",
        "  for x in f: diccionario.extend(x)\n",
        " \n",
        "\n",
        "diccionario=Counter(diccionario)\n",
        "diccionario_raw=[k for k,v in diccionario.items() if v>2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca0oUoFHdD3_"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(\"loquentur\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C5-DGXqZwpj"
      },
      "outputs": [],
      "source": [
        "for l in c[\"block\"][3:12]:\n",
        "  for w in word_tokenize(l):\n",
        "    if w not in diccionario_raw and not w[0].isupper():\n",
        "    \n",
        "      liste=[x for x in model.wv.most_similar(w) if (x[0][0]==w[0] and x[1]>0.85 )]\n",
        "      print(w, liste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r1894_kDHkt"
      },
      "outputs": [],
      "source": [
        "gram=3\n",
        "all_texts=[]\n",
        "for k,v in dict_registres.items():\n",
        "  text=[x[-1] for x in v if len(x[-1])>0]\n",
        "  text=\" \".join(text).split()\n",
        "  text=[\" \".join(text[i:i+gram]) for i in range(len(text)-gram+1)]\n",
        "  all_texts.extend(text)\n",
        "\n",
        "all_texts_txt=\"\\n\".join(all_texts)\n",
        "\n",
        "with open(\"/content/all_transcribed.txt\", 'w', encoding=\"utf-8\") as f:\n",
        "  f.write(all_texts_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD0W0Gz8CRq0"
      },
      "outputs": [],
      "source": [
        "len(all_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDVQjv57CRtV"
      },
      "outputs": [],
      "source": [
        "all_texts[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXrU8UrnW46j"
      },
      "outputs": [],
      "source": [
        "candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6PFn92NXGid"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "#error=\"decembris suit festum\"\n",
        "error=\"festum betorum Petri\"\n",
        "candidates=[]\n",
        "\n",
        "for s in all_texts:\n",
        "  s=s.split()\n",
        "  if fuzz.ratio(error.split()[0], s[0])>90 and fuzz.ratio(error.split()[2], s[2])>90 and fuzz.ratio(error.split()[1], s[1])>80:\n",
        "    candidates.append(\" \".join(s))\n",
        "\n",
        "candidates=[(k,v) for k,v in Counter(candidates).items()]\n",
        "candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "print(*candidates, sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL9i32HYar9r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaxT-0UNasAR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"./gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0ZwZmFVXPfK"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "model = FastText.load_fasttext_format('/content/gdrive/MyDrive/Colab Notebooks/NER_multi/fasttext_multi/all_multi_fasttext_150dim_18_03_2022.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kTH7vnKayns"
      },
      "outputs": [],
      "source": [
        "searched_word='alliquando'\n",
        "\n",
        "liste=[x for x in model.wv.most_similar(searched_word) if x[0][0]==searched_word[0]]\n",
        "liste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkgeSr_aayqZ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import nltk\n",
        "from collections import Counter\n",
        "diccionario_trans=[]\n",
        "for k,v in dict_registres.items():\n",
        "  a,b=transformation(k)\n",
        "  c=copy.deepcopy(b)\n",
        "  c=\" \".join(c[\"block\"])\n",
        "  c=word_tokenize(c)\n",
        "  diccionario_trans.extend(c)\n",
        "diccionario_trans=Counter(diccionario_trans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N35N-LX-l3LL"
      },
      "outputs": [],
      "source": [
        "inter=(diccionario_raw^diccionario_trans.keys())&diccionario_trans.keys()\n",
        "# operador ^ elimina los elementos comunes en ambos, es decir, existentes en el diccionario\n",
        "#luego & busca la interseccion, es decir retiene solo los elementos de dict trans no existentes en dict raw.\n",
        "inter={k:diccionario_trans[k] for k in list(inter)}\n",
        "inter={k: v for k, v in sorted(inter.items(), key=lambda item: item[1], reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yXZEJBttTBf"
      },
      "outputs": [],
      "source": [
        "diccionario_trans_b=[]\n",
        "for k,v in dict_registres.items():\n",
        "  item=[]\n",
        "  for line in v:\n",
        "    if len(v)>2:\n",
        "      item.append(line[-1])\n",
        "  item=word_tokenize(\" \".join(item))\n",
        "  diccionario_trans_b.extend(item)\n",
        "\n",
        "diccionario_trans_b=Counter(diccionario_trans_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7SaXqkaxs7ES"
      },
      "outputs": [],
      "source": [
        "inter_b=(diccionario_trans_b.keys()^diccionario_trans.keys())&diccionario_trans.keys()\n",
        "# operador ^ elimina los elementos comunes en ambos, es decir, existentes en el diccionario\n",
        "#luego & busca la interseccion, es decir retiene solo los elementos de dict trans no existentes en dict raw.\n",
        "inter_b={k:diccionario_trans[k] for k in list(inter_b)}\n",
        "inter_b={k: v for k, v in sorted(inter_b.items(), key=lambda item: item[1], reverse=True)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXX5RNbU2YQb"
      },
      "outputs": [],
      "source": [
        "inter_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MJnJGM6s3LY"
      },
      "outputs": [],
      "source": [
        "counter=0\n",
        "for i,k in enumerate(inter_b.keys()):\n",
        "  if i<2000:\n",
        "    print(i, k, inter_b[k])\n",
        "    counter+=inter_b[k]\n",
        "print(counter/5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHcyAuQfmPGs"
      },
      "outputs": [],
      "source": [
        "inter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lnoss9kktW3"
      },
      "outputs": [],
      "source": [
        "def count_intersections(lst1, lst2):\n",
        "    c1 = Counter(lst1)\n",
        "    c2 = Counter(lst2)\n",
        "    return { k: min(c1[k], c2[k]) for k in c1.keys() & c2.keys() }\n",
        "\n",
        "inter=count_intersections(diccionario_raw, diccionario_trans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp1_PigTaytV",
        "outputId": "0f5ac7e0-b31d-4f49-fb92-b9eb85071838"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys([])"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diccionario_trans_b.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yP1vpAorVYjm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAEu8cuYVYmb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln3y_AJ6GjSF"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "#model = fasttext.train_unsupervised('/content/pretrain_src1_all_small.txt', minn=2, maxn=5, dim=100, epoch=10, verbose=1)\n",
        "model = fasttext.train_unsupervised('/content/all_transcribed.txt', minn=2, maxn=5, dim=100, epoch=10, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAxxCX4dH-Pe"
      },
      "outputs": [],
      "source": [
        "model.get_nearest_neighbors(\"decembris fuit festum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SplxrmrPH-dp"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tagged_data = [TaggedDocument(words=[x for x in word_tokenize(y)], tags=[str(i)]) for i,y in enumerate(all_texts)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2TMKItvKw7f"
      },
      "outputs": [],
      "source": [
        "#modeling vectors, this can take a while\n",
        "max_epochs = 20\n",
        "vec_size = 30\n",
        "alpha = 0.025\n",
        "\n",
        "model_d2v = Doc2Vec(vector_size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1, window=10, workers=4)\n",
        "  \n",
        "model_d2v.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model_d2v.train(tagged_data,\n",
        "                total_examples=model_d2v.corpus_count,\n",
        "                epochs=model_d2v.iter)\n",
        "    # decrease the learning rate\n",
        "    model_d2v.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model_d2v.min_alpha = model_d2v.alpha\n",
        "\n",
        "print(\"end modelization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNqfKZ7gG_dB"
      },
      "outputs": [],
      "source": [
        "model_d2v.save(\"encpos_doc2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PlrCYDMVouV"
      },
      "outputs": [],
      "source": [
        "model_d2v.most_similar([\"decembris\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd5EYhreMA21"
      },
      "outputs": [],
      "source": [
        "doc_embedding = model.encode([bloc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehy4lnmtMA5I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY8Xx0XPGjUX"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "test=[]\n",
        "diccionario=[]\n",
        "#with open(\"pretrain_src1_all_small.txt\") as f:\n",
        "with open(\"training_txt_greek.txt\") as f:\n",
        "  f=f.readlines()\n",
        "  f=[x[:-1] for x in f ]\n",
        "  for x in f:\n",
        "    if x.startswith(\"TE_L\") or x.startswith(\"TE_P\"):\n",
        "      test.append(x.split(\" \", 1)[1:])\n",
        "    else:\n",
        "      if x.startswith(\"VA_T\"):\n",
        "        continue\n",
        "      else:\n",
        "        diccionario.append(x.split(\" \", 1)[1:])\n",
        "\n",
        "test=[test[_:_+2] for _ in range(0, len(test), 2)]\n",
        "diccionario=Counter([y for x in [x[0].split() for x in diccionario] for y in x])\n",
        "diccionario_raw=[k for k,v in diccionario.items() if v>2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAbLAV6oGjW4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJA-JCuRccse"
      },
      "outputs": [],
      "source": [
        "!pip install pie-extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieaj-1ZPcqwg"
      },
      "outputs": [],
      "source": [
        "!pie-extended download lasla\n",
        "!pie-extended install-addons lasla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYnZ8TewdOaJ"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pie_extended.cli.utils import get_tagger, get_model, download\n",
        "\n",
        "# model_path allows you to override the model loaded by another .tar\n",
        "model_name = \"lasla\"\n",
        "tagger = get_tagger(model_name, batch_size=256, device=\"cpu\", model_path=None)\n",
        "tagged_sentences=[]\n",
        "\n",
        "sentences: List[str] = [\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \"]\n",
        "# Get the main object from the model (: data iterator + postprocesor\n",
        "from pie_extended.models.lasla.imports import get_iterator_and_processor\n",
        "for sentence_group in sentences:\n",
        "    iterator, processor = get_iterator_and_processor()\n",
        "    tagged_sentences.extend([list(x.values())[:3] for x in tagger.tag_str(sentence_group, iterator=iterator, processor=processor)])\n",
        "    #print(tagger.tag_str(sentence_group, iterator=iterator, processor=processor) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs-pKijBccuN"
      },
      "outputs": [],
      "source": [
        "list(tagged_sentences[0].values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYq41os7elv9"
      },
      "outputs": [],
      "source": [
        "tagged_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8KFszDCflnc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "tagged_sentences=[]\n",
        "\n",
        "pages_list=list(dict_registres.keys())\n",
        "dict_omnia_endp={}\n",
        "nosketch_vertical=\"\" # the vertical file is a classical column txt/not formatted file but supporting html tags\n",
        "\n",
        "#with open('dict_omnia_endp_keywords.json', 'r') as json_file: dict_omnia_endp_keywords = json.load(json_file)\n",
        "\n",
        "random.shuffle(pages_list) #optional\n",
        "\n",
        "for i, k in enumerate(pages_list):\n",
        "  if \"FRAN_0393\" in k:\n",
        "    keywords_c=\"Unclassed\"\n",
        "    a,b=transformation(k)\n",
        "    c=copy.deepcopy(b)\n",
        "    #texto=\" \".join(c[\"block\"])\n",
        "    \n",
        "    page_id=int(k.split(\"_\")[2].lstrip(\"0\"))\n",
        "    #print(numero, k)\n",
        "\n",
        "    iterator, processor = get_iterator_and_processor()\n",
        "    # The Omnia treetagger tagging\n",
        "    for kk,vv in c.items(): #iteration occurs inside the content of each detected page region\n",
        "      vv=\" \".join(vv)\n",
        "      if len(vv)>1:\n",
        "        tagged_sentences.extend([list(x.values())[:3] for x in tagger.tag_str(vv, iterator=iterator, processor=processor)])\n",
        "  \n",
        "\n",
        "  if i%10==0:\n",
        "    print(i)\n",
        "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ShNQy7cflqc"
      },
      "outputs": [],
      "source": [
        "tagged_sentences[:100]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}